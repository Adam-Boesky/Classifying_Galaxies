{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 {\n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #63ACBE;\n",
       "    color: black;\n",
       "}\n",
       "h2 {\n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #f8b4ab;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #ffd0d0;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #63ACBE;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc {\n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 {\n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD;\n",
       "    color: black;\n",
       "}\n",
       "span.emph {\n",
       "\tcolor: #601A4A;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import (LogisticRegression, LinearRegression)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Silence FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement (Caleb)\n",
    "\n",
    "1. Determine and train the best model for classifying galaxies on morphological type (TT response variable) and on Hubble type (spiral, lenticular, irregular), and determine which physical parameters are most important for classification.\n",
    "\n",
    "2. Recovering known astronomical relationships (e.g. Hubble Constant) from the data and giving a physical sense for our findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "processed_galaxies_df = pd.read_csv('../data/preprocessed_data.csv')\n",
    "processed_galaxies_df= processed_galaxies_df.drop(['Unnamed: 0', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data that includes indicator for imputed values\n",
    "missingness = pd.read_csv('../data/missingness.csv')\n",
    "missingness= missingness.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (John, Adam, Caleb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Discussion (John, Adam)\n",
    "\n",
    "Which models could work and why?\n",
    "\n",
    "* __Multiple Logistic Regression with LASSO Regularization__: Useful in multiclass problems on non-ordinal, categorical response variables. Good for interpretability since we get coefficient estimates to relate predictors and the log-odds of belonging to a specific class; when used with LASSO regularization, parameters are shrunk to zero, preserving the interpretability.\n",
    "\n",
    "* __Random Forest__: Useful in multi-class, classification problems. Random forests help reduce collinearity between predictors with the randomness involved in decision tree splits. They may also find more complex decision boundaries than just logistic regression due to the non-parametric approach to modeling. Lastly, there is a very straightforward method for feature importance.\n",
    "\n",
    "* __Gradient Boosting__: Useful in multi-class, classification problems. GB provides good performance with an inbalanced data sets because it trains itself on the residuals; i.e. underrepresented data will have higher error, and thus the model will focus more on those data points on subsequent interations. Lastly, GB may also find more complex decision boundaries than just logistic regression due to the non-parametric approach to modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hubble Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "1. Get Hubble class (spiral, irregular, lenticular)\n",
    "2. Split into train and test sets\n",
    "3. Standardization of quantitative predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion Lenticular: 0.197\n",
      "Portion Spiral: 0.124\n",
      "Portion Irregular: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/7r72r1kn6x596_mb7pz6xxfc0000gn/T/ipykernel_86581/1050591817.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_galaxies_df['hubble_class'][(processed_galaxies_df['TT'] >= 0) & (processed_galaxies_df['TT'] <= 7)] = 1\n",
      "/var/folders/b7/7r72r1kn6x596_mb7pz6xxfc0000gn/T/ipykernel_86581/1050591817.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_galaxies_df['hubble_class'][processed_galaxies_df['TT'] >  7] = 2\n"
     ]
    }
   ],
   "source": [
    "# Define a variable 'gal_type' that stores the type of galaxy \n",
    "processed_galaxies_df['hubble_class'] = 0\n",
    "processed_galaxies_df['hubble_class'][(processed_galaxies_df['TT'] >= 0) & (processed_galaxies_df['TT'] <= 7)] = 1\n",
    "processed_galaxies_df['hubble_class'][processed_galaxies_df['TT'] >  7] = 2\n",
    "## ===== CODE: (0, 1, 2) = (lenticular, spiral, irregular) ===== ##\n",
    "\n",
    "# Print the percent class breakdowns for each Hubble class\n",
    "print(\"Portion Lenticular:\", round(np.mean(processed_galaxies_df['hubble_class'] == 0), 3))\n",
    "print(\"Portion Spiral:\", round(np.mean(processed_galaxies_df['hubble_class'] == 1), 3))\n",
    "print(\"Portion Irregular:\", round(np.mean(processed_galaxies_df['hubble_class'] == 2), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the list of the quantitative predictors\n",
    "quant_preds = processed_galaxies_df.select_dtypes(include=['number']).columns.drop(['TT', 'hubble_class'])\n",
    "\n",
    "# Split into train and test sets for hubble class\n",
    "X_train, X_test, y_train_hubble, y_test_hubble = train_test_split(processed_galaxies_df[quant_preds], processed_galaxies_df['hubble_class'], random_state=109, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Instantiate a scaler\n",
    "standardscaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Standardize the data\n",
    "X_train_std = standardscaler.transform(X_train)\n",
    "X_test_std = standardscaler.transform(X_test)\n",
    "\n",
    "# Define a dictionary to store the train and test accuracies of the optimal models throughout this notebook\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Logistic Regression with LASSO Regularization, Tuned with CV (k=25), Not-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 11.288378916846883}\n",
      "Best Cross-Validation Score: 0.8723809523809523\n",
      "Logreg Train Accuracy: 0.8914616497829233\n",
      "Logreg Test Accuracy: 0.8901734104046243\n"
     ]
    }
   ],
   "source": [
    "# Parameters to tune with SMOTE and CV\n",
    "param_grid = {'C': np.logspace(-4, 4, 20)}\n",
    "\n",
    "# Base classifier\n",
    "logreg_base = LogisticRegression(penalty='l1', solver='liblinear', multi_class='ovr', random_state=109)\n",
    "\n",
    "# Perform the cross_validation\n",
    "grid_search = GridSearchCV(logreg_base, param_grid, cv=25, scoring='accuracy').fit(X_train_std, y_train_hubble)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Fit the best model from the cross validation and compute the train and test accuracies\n",
    "best_logreg = LogisticRegression(penalty='l1', solver='liblinear', C=grid_search.best_params_['C']).fit(X_train_std, y_train_hubble)\n",
    "best_logreg_train_acc = best_logreg.score(X_train_std, y_train_hubble)\n",
    "best_logreg_test_acc = best_logreg.score(X_test_std, y_test_hubble)\n",
    "print(\"Logreg Train Accuracy:\", best_logreg_train_acc)\n",
    "print(\"Logreg Test Accuracy:\", best_logreg_test_acc)\n",
    "\n",
    "# Store accuracies in the accuracies dictionary\n",
    "accuracies['Logreg'] = {\n",
    "    'train': best_logreg_train_acc,\n",
    "    'test': best_logreg_test_acc,\n",
    "    'val': grid_search.best_score_\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Logistic Regression with LASSO Regularization, Tuned with CV (k=25), With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'log_reg__C': 4.281332398719396}\n",
      "Best Cross-Validation Score: 0.8577777777777778\n",
      "Logistic-SMOTE Train Accuracy: 0.8914616497829233\n",
      "Logistic-SMOTE Test Accuracy: 0.8959537572254336\n"
     ]
    }
   ],
   "source": [
    "# Parameters to tune with CV\n",
    "param_grid = {'log_reg__C': np.logspace(-4, 4, 20)}\n",
    "\n",
    "# Pipeline for CV with SMOTE\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=109)),\n",
    "    ('log_reg', LogisticRegression(penalty='l1', solver='liblinear', multi_class='ovr', random_state=109))\n",
    "])    \n",
    "\n",
    "# Perform the cross_validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=25, scoring='accuracy').fit(X_train_std, y_train_hubble)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Fit the best model from the cross validation and compute the train and test accuracies\n",
    "best_logreg = LogisticRegression(penalty='l1', solver='liblinear', C=grid_search.best_params_['log_reg__C']).fit(X_train_std, y_train_hubble)\n",
    "best_logreg_train_acc_SMOTE = best_logreg.score(X_train_std, y_train_hubble)\n",
    "best_logreg_test_acc_SMOTE = best_logreg.score(X_test_std, y_test_hubble)\n",
    "print(\"Logistic-SMOTE Train Accuracy:\", best_logreg_train_acc_SMOTE)\n",
    "print(\"Logistic-SMOTE Test Accuracy:\", best_logreg_test_acc_SMOTE)\n",
    "\n",
    "\n",
    "# Store accuracies in the accuracies dictionary\n",
    "accuracies['Logreg-SMOTE'] = {\n",
    "    'train': best_logreg_train_acc_SMOTE,\n",
    "    'test': best_logreg_test_acc_SMOTE,\n",
    "    'val': grid_search.best_score_\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest, Tuned with CV (k=25), Not-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 11, 'min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.8984656084656085\n",
      "RF Train Accuracy: 1.0\n",
      "RF Test Accuracy: 0.8786127167630058\n"
     ]
    }
   ],
   "source": [
    "# Parameters to tune with SMOTE and CV\n",
    "param_grid = {'max_depth': np.arange(5, 15, 1), 'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Base classifier\n",
    "rf_base = RandomForestClassifier(n_estimators=100, random_state=109)\n",
    "\n",
    "# Perform the cross_validation\n",
    "grid_search = GridSearchCV(rf_base, param_grid, cv=25, scoring='accuracy').fit(X_train_std, y_train_hubble)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the best model based on the CV, and calculate the train and test accuracies of best estimator\n",
    "best_rf = RandomForestClassifier(n_estimators=100, max_depth=best_params['max_depth'], criterion=best_params['criterion'], min_samples_split=best_params['min_samples_split'], random_state=109).fit(X_train_std, y_train_hubble)\n",
    "best_rf_train_acc = best_rf.score(X_train_std, y_train_hubble)\n",
    "best_rf_test_acc = best_rf.score(X_test_std, y_test_hubble)\n",
    "print(\"RF Train Accuracy:\", best_rf_train_acc)\n",
    "print(\"RF Test Accuracy:\", best_rf_test_acc)\n",
    "\n",
    "accuracies['random_forest'] = {\n",
    "    'train': best_rf_train_acc,\n",
    "    'test' : best_rf_test_acc,\n",
    "    'val': grid_search.best_score_\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest, Tuned with CV (k=25), With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__criterion': 'entropy', 'classifier__max_depth': 9, 'classifier__min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.8912698412698412\n",
      "RF-SMOTE Train Accuracy: 1.0\n",
      "RF-SMOTE Test Accuracy: 0.8786127167630058\n"
     ]
    }
   ],
   "source": [
    "# Parameters to tune with SMOTE and CV\n",
    "param_grid = {'classifier__max_depth': np.arange(5, 15, 1), 'classifier__criterion': ['gini', 'entropy'], 'classifier__min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Pipeline for CV with SMOTE\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=109)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=109))\n",
    "])    \n",
    "\n",
    "# Perform the cross_validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=25, scoring='accuracy').fit(X_train_std, y_train_hubble)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the best model based on the CV, and calculate the train and test accuracies of best estimator\n",
    "best_rf = RandomForestClassifier(n_estimators=100, max_depth=best_params['classifier__max_depth'], criterion=best_params['classifier__criterion'], min_samples_split=best_params['classifier__min_samples_split'], random_state=109).fit(X_train_std, y_train_hubble)\n",
    "best_rf_train_acc_SMOTE = best_rf.score(X_train_std, y_train_hubble)\n",
    "best_rf_test_acc_SMOTE = best_rf.score(X_test_std, y_test_hubble)\n",
    "print(\"RF-SMOTE Train Accuracy:\", best_rf_train_acc_SMOTE)\n",
    "print(\"RF-SMOTE Test Accuracy:\", best_rf_test_acc_SMOTE)\n",
    "\n",
    "accuracies['random_forest-SMOTE'] = {\n",
    "    'train': best_rf_train_acc_SMOTE,\n",
    "    'test' : best_rf_test_acc_SMOTE,\n",
    "    'val' : grid_search.best_score_\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Gradient Boosting, Tuned with CV (k=10), Not-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}\n",
      "Best Cross-Validation Score: 0.9102691511387162\n",
      "GradBoost Train Accuracy: 1.0\n",
      "GradBoost Test Accuracy: 0.9075144508670521\n"
     ]
    }
   ],
   "source": [
    "# Parameters to tune with CV\n",
    "param_grid = {\"max_depth\": [2, 3, 4], \"learning_rate\": [0.001, 0.01, 0.1], \"n_estimators\": [50, 100, 150]}\n",
    "\n",
    "# base boosting classifier\n",
    "boost_base = GradientBoostingClassifier(random_state=109)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(boost_base, param_grid, cv=10, scoring='accuracy', verbose=1).fit(X_train_std, y_train_hubble)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the best model based on the CV, and calculate the train and test accuracies of best estimator\n",
    "best_booster = GradientBoostingClassifier(max_depth=best_params['max_depth'], learning_rate=best_params['learning_rate'], n_estimators=best_params['n_estimators'], random_state=109).fit(X_train_std, y_train_hubble)\n",
    "best_booster_train_acc = best_booster.score(X_train_std, y_train_hubble)\n",
    "best_booster_test_acc = best_booster.score(X_test_std, y_test_hubble)\n",
    "print(\"GradBoost Train Accuracy:\", best_booster_train_acc)\n",
    "print(\"GradBoost Test Accuracy:\", best_booster_test_acc)\n",
    "\n",
    "accuracies['skl_grad_boost'] = {\n",
    "    'train': best_booster_train_acc,\n",
    "    'test' : best_booster_test_acc,\n",
    "    'val' : grid_search.best_score_\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Gradient Boosting, Tuned with CV (k=10), With-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n",
      "Best Parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 150}\n",
      "Best Cross-Validation Score: 0.916024844720497\n",
      "GradBoost Train Accuracy: 1.0\n",
      "GradBoost Test Accuracy: 0.9017341040462428\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \"classifier__max_depth\": [2, 3, 4], \"classifier__learning_rate\": [0.001, 0.01, 0.1], \"classifier__n_estimators\": [50, 100, 150]}\n",
    "\n",
    "# Pipeline for CV with SMOTE\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=109)),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=109))\n",
    "])\n",
    "\n",
    "# Grid serach CV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy', verbose=1).fit(X_train_std, y_train_hubble)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the best model based on the CV, and calculate the train and test accuracies of best estimator\n",
    "best_booster = GradientBoostingClassifier(max_depth=best_params['classifier__max_depth'], learning_rate=best_params['classifier__learning_rate'], n_estimators=best_params['classifier__n_estimators'], random_state=109).fit(X_train_std, y_train_hubble)\n",
    "best_booster_train_acc_SMOTE = best_booster.score(X_train_std, y_train_hubble)\n",
    "best_booster_test_acc_SMOTE = best_booster.score(X_test_std, y_test_hubble)\n",
    "print(\"GradBoost Train Accuracy:\", best_booster_train_acc_SMOTE)\n",
    "print(\"GradBoost Test Accuracy:\", best_booster_test_acc_SMOTE)\n",
    "\n",
    "accuracies['skl_grad_boost-SMOTE'] = {\n",
    "    'train': best_booster_train_acc_SMOTE,\n",
    "    'test' : best_booster_test_acc_SMOTE,\n",
    "    'val': grid_search.best_score_\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the Accuracies of Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logreg</th>\n",
       "      <td>0.891</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logreg-SMOTE</th>\n",
       "      <td>0.891</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest-SMOTE</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skl_grad_boost</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skl_grad_boost-SMOTE</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train   test    val\n",
       "Logreg                0.891  0.890  0.872\n",
       "Logreg-SMOTE          0.891  0.896  0.858\n",
       "random_forest         1.000  0.879  0.898\n",
       "random_forest-SMOTE   1.000  0.879  0.891\n",
       "skl_grad_boost        1.000  0.908  0.910\n",
       "skl_grad_boost-SMOTE  1.000  0.902  0.916"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(accuracies).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ay98",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
